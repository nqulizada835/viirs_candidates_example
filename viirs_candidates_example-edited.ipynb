{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe9592f3-0e1d-40ac-af59-7eb7d05c463d",
   "metadata": {},
   "source": [
    "Search for granules on the cloud for a box of interest, run custom candidate fire pixel extraction, map I4 + custom candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9501ee4f-413b-4d7b-b57b-f020fafd2b1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#conda activate /projects/myenvs/candidates-env\n",
    "import xarray as xr\n",
    "import earthaccess\n",
    "from earthaccess import Auth, Store, DataCollections, DataGranules\n",
    "import datetime as dt\n",
    "import time\n",
    "import os\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import folium\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import pandas as pd\n",
    "import rioxarray\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e909ee04-4f90-4725-b63b-88dd8e8ab6ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_dir = '/projects/shared-buckets/qulizad/scripts/outputs/canada/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f65a4ad-753a-4723-a7e9-46e1ac31be48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_dir(fire_name):\n",
    "    if not os.path.isdir(output_dir):\n",
    "        os.mkdir(output_dir)\n",
    "\n",
    "    if not os.path.isdir(output_dir + fire_name):\n",
    "        os.mkdir(output_dir + fire_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf429461-299f-4e02-bdde-f288fe4e7a3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fetch_data(satellite):\n",
    "    \n",
    "    if satellite=='SNPP': products = ['VNP02IMG','VNP03IMG']\n",
    "    elif satellite=='NOAA20': products = ['VJ102IMG','VJ103IMG']\n",
    "    \n",
    "    s3_links = {}\n",
    "    files = {}\n",
    "    for product in products:\n",
    "        #query for granules - by bounding box or point\n",
    "        Query = DataGranules().short_name(product).bounding_box(EXTENT[0],EXTENT[1],EXTENT[2],EXTENT[3]).temporal(START,END)\n",
    "\n",
    "        cloud_granules = Query.get(800) #first 800 results\n",
    "\n",
    "        #save granule URLs to list\n",
    "        s3_links[product] = []\n",
    "        for granule in cloud_granules:\n",
    "            s3_links[product].extend(granule.data_links(access=\"direct\"))\n",
    "        s3_links[product] = sorted(s3_links[product]) \n",
    "        files[product] = store.open(s3_links[product], provider=\"LAADS\")\n",
    "\n",
    "    return s3_links, files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6563793-5ae1-4a99-9749-6c6cba09a431",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_fire_algorithm():\n",
    "    all_dets = pd.DataFrame()\n",
    "    \n",
    "    products = list(s3_links.keys())\n",
    "\n",
    "    for i in range(len(products[0])): #VNP02IMG or VJ102IMG\n",
    "        timestamp = s3_links[products[0]][i].split('.')[1:3]\n",
    "        print(timestamp)\n",
    "        year = timestamp[0][1:5]\n",
    "        day = timestamp[0][5:8]\n",
    "        time = timestamp[1]\n",
    "        date = dt.datetime.strptime(year+day, '%Y%j').strftime('%b %d') \n",
    "        acq_datetime = dt.datetime.strptime(year+day+time[:2]+time[2:], '%Y%j%H%M').strftime('%Y-%m-%d %H:%M:00 +00:00') \n",
    "        daytime = int(time) > 1500 #depends on timezone\n",
    "        \n",
    "        try:\n",
    "            #open VNP03IMG geolocation\n",
    "            geo = xr.open_dataset(files[products[1]][i], engine='h5netcdf', group='geolocation_data')\n",
    "            lon = geo['longitude'][:]\n",
    "            lat = geo['latitude'][:]\n",
    "\n",
    "            scene = (lon > EXTENT[0]) & (lon < EXTENT[2]) & (lat > EXTENT[1]) & (lat < EXTENT[3])\n",
    "\n",
    "            #crop down the datasets for memory \n",
    "            indices = np.where(scene)\n",
    "            x0 = indices[0].min()\n",
    "            x1 = indices[0].max()\n",
    "            y0 = indices[1].min()\n",
    "            y1 = indices[1].max()\n",
    "\n",
    "            lon = lon[x0:x1, y0:y1]\n",
    "            lat = lat[x0:x1, y0:y1]\n",
    "\n",
    "            #get VNP02IMG science data, i1-i5 bands\n",
    "            data = xr.open_dataset(files[products[0]][i], engine='h5netcdf', group='observation_data')\n",
    "            data = data.sel(number_of_lines=slice(x0,x1), number_of_pixels=slice(y0,y1))\n",
    "\n",
    "            i4 = data['I04'] #xarray already encodes the scale factor and offset\n",
    "            scale = data.I04.encoding['scale_factor']\n",
    "            offset = data.I04.encoding['add_offset']\n",
    "            i4 = (i4[:,:] - offset) / scale #return to raw values to use lookup table to temperature\n",
    "            i4 = i4.astype(int)\n",
    "            i4_bt = data['I04_brightness_temperature_lut'][:]\n",
    "            i4_bt = i4_bt[i4]\n",
    "\n",
    "            i5 = data['I05']\n",
    "            scale = data.I05.encoding['scale_factor']\n",
    "            offset = data.I05.encoding['add_offset']\n",
    "            i5 = (i5[:,:] - offset) / scale\n",
    "            i5 = i5.astype(int)\n",
    "            i5_bt = data['I05_brightness_temperature_lut'][:]\n",
    "            i5_bt = i5_bt[i5]\n",
    "\n",
    "        except:\n",
    "            print('error with file or does not exist',timestamp)\n",
    "            continue\n",
    "\n",
    "\n",
    "        if daytime:\n",
    "            clouds = (i5_bt < 265) | ((data.I01+data.I02 > 0.9) & (i5_bt < 295)) | ((data.I01+data.I02 > 0.7) & (i5_bt < 285)) \n",
    "            water = (data.I01 > data.I02) & (data.I02 > data.I03)\n",
    "            saturated = (i4_bt==367) & (data.I04_quality_flags==9) & (data.I05_quality_flags==0) & (i5_bt > 290) & (data.I01+data.I02 < 0.7)\n",
    "            folded = (i4_bt-i5_bt < 0) & (i5_bt > 325) & (data.I05_quality_flags==0)\n",
    "            bg_fires = ((i4_bt > 335) & (i4_bt-i5_bt > 30)) | (folded)\n",
    "            bright = (data.I03 > 0.3) & (data.I03 > data.I02) & (data.I02 > 0.25) & (i4_bt <335)\n",
    "            #glint = come back to this\n",
    "            candidates = (~bright) & (i4_bt > 325) & (i4_bt-i5_bt > 25) #double check this isn't contextual\n",
    "\n",
    "            fires =  (saturated | folded | bg_fires | candidates) #~clouds & ~water &\n",
    "\n",
    "        else: #nighttime\n",
    "            clouds = (i5_bt < 265) & (i4_bt < 295)\n",
    "            unequivocal = (i4_bt > 320) & (data.I04_quality_flags==0)\n",
    "            saturated = (i4_bt==367) & (data.I04_quality_flags==9) & (data.I05_quality_flags==0)\n",
    "            folded = ((i4_bt-i5_bt < 0) & (i5_bt > 310) & (data.I05_quality_flags==0)) | ((i4_bt > 208) & (i5_bt > 335))\n",
    "            bg_fires = ((i4_bt > 300) & (i4_bt-i5_bt > 10)) | (folded)\n",
    "            #bright = water #all false\n",
    "            candidates = (i4_bt > 295) & (i4_bt-i5_bt > 10) \n",
    "\n",
    "            fires = (unequivocal | saturated | folded | bg_fires | candidates) #~clouds & \n",
    "\n",
    "\n",
    "        #build pandas table for exporting, following VIIRS L2 columns\n",
    "        i_dets = pd.DataFrame() #copy of master table just for this swath\n",
    "        i_dets['longitude'] = list(np.array(lon)[fires])\n",
    "        i_dets['latitude'] = list(np.array(lat)[fires])\n",
    "        i_dets['acq_date'] = dt.datetime.strptime(year+day, '%Y%j').strftime('%Y/%m/%d') \n",
    "        i_dets['acq_time'] = time\n",
    "        i_dets['acq_datetime'] = acq_datetime\n",
    "\n",
    "        #crop down to defined extent\n",
    "        i_dets = i_dets[(i_dets.longitude > EXTENT[0]) & (i_dets.longitude < EXTENT[2]) & (i_dets.latitude > EXTENT[1]) & (i_dets.latitude < EXTENT[3])]\n",
    "\n",
    "        #FIGURE ----------------\n",
    "        fig = plt.figure(figsize=(11,8))\n",
    "\n",
    "        ax = fig.add_subplot(121, projection=ccrs.Miller())\n",
    "        ax.set_extent([EXTENT[0],EXTENT[2],EXTENT[1],EXTENT[3]])\n",
    "        plot = ax.pcolormesh(lon, lat, i4_bt, vmin=250, vmax=360, cmap='plasma', transform=ccrs.PlateCarree())\n",
    "        cbar = plt.colorbar(plot, orientation='horizontal', shrink=0.6, pad=0.01, extend='both', ax=ax)\n",
    "        cbar.ax.tick_params(labelsize=13)\n",
    "        cbar.set_label('I4 brightness temperature (K)', size=13)\n",
    "        ax.set_title(f'{s} {date} {time}h UTC') \n",
    "             \n",
    "        ax2 = fig.add_subplot(122, projection=ccrs.Miller())\n",
    "        ax2.set_extent([EXTENT[0],EXTENT[2],EXTENT[1],EXTENT[3]])\n",
    "        plot = ax2.pcolormesh(lon, lat, i4_bt, vmin=250, vmax=360, cmap='plasma', transform=ccrs.PlateCarree())\n",
    "        cbar = plt.colorbar(plot, orientation='horizontal', shrink=0.6, pad=0.01, extend='both', ax=ax2)\n",
    "        cbar.ax.tick_params(labelsize=13)\n",
    "        cbar.set_label('I4 brightness temperature (K)', size=13)\n",
    "        ax2.scatter(i_dets.longitude, i_dets.latitude, c='1', s=1, transform=ccrs.Geodetic())\n",
    "        ax2.text(0.35, 0.9, 'Potential fire pixels', c='white', transform = ax2.transAxes)\n",
    "        figimgs = (f'{output_dir}/{fire_name}/{timestamp[0]}-{timestamp[1]}_{s}.png')\n",
    "        plt.savefig(figimgs, dpi=100)\n",
    "        plt.close()\n",
    "        all_dets = pd.concat([all_dets, i_dets])\n",
    "    \n",
    "    #save csv with filename as the timestamp range\n",
    "    filecsv = (f'{output_dir}/{timestamp[0]}_{s}.csv')\n",
    "    all_dets.to_csv(filecsv, index=False)\n",
    "\n",
    "    print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c94e1bf-14f5-455a-9620-755829c782f7",
   "metadata": {},
   "source": [
    "Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444807f1-9b66-44da-ba43-2d22a6e2a35e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#start = time.time()\n",
    "#start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d58fa7a-0892-45a0-a90a-a119524081e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#read in pyroCB catalog\n",
    "file = f'/projects/shared-buckets/qulizad/NRL_pyroCb_inventory_2023_v1.csv'\n",
    "df = pd.read_csv(file, header=0, usecols=[\"datetime\", \"fire_name\", \"region\", \"lat\", \"lon\"])\n",
    "rslt_df = df.loc[(df['region'] == 'Canada')]  #.reset_index(drop=True) #subsetting to Canada\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7d9423-3d93-4cc7-8ba6-613b10bfd533",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rslt_df.loc[25] ## only prints up to not including 10th record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befb693f-8454-4571-8d58-ff40de92b9ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#initiate cloud session - need to reauthenticate every hour :(\n",
    "\n",
    "auth = Auth() \n",
    "#auth.login(strategy=\"interactive\", persist=True) #RUN THIS THE FIRST TIME\n",
    "auth.login(strategy=\"netrc\") #read credentials from previously saved ~/.netrc file\n",
    "\n",
    "store = Store(auth)\n",
    "fs = store.get_s3fs_session('LAADS') #daac or provider name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9b1ac2-cd70-4ad0-8af6-a7c7abdfdfc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#loop through all pyroCBs, run methods to make output directories, fetch data, and generate candidate fires\n",
    "\n",
    "#%time\n",
    "# %time is not the same as %%time because the former only see's how long the current \n",
    "#line takes to execute, whereas the latter checks the how the current line and \n",
    "#following lines take to execute\n",
    "#[:] is the array slice syntax for every element in the array\n",
    "\n",
    "#start time \n",
    "\n",
    "for i in rslt_df.index[37:44]:\n",
    "    #if i%5==0: #reauthenticate\n",
    "    \n",
    "    #current & elapsed = ...\n",
    "    #if elapsed > 0.98: #reauthenticate, reset \"start\" time\n",
    "        \n",
    "    fire_name = rslt_df.loc[i, 'fire_name']\n",
    "    lat = rslt_df.loc[i, 'lat']\n",
    "    lon = rslt_df.loc[i, 'lon']\n",
    "    dat = rslt_df.loc[i, 'datetime']\n",
    "    date_object = dt.datetime.strptime(dat, \"%m/%d/%y %H:%M\")    \n",
    "    print(i, fire_name, date_object)\n",
    "    \n",
    "    start_date = date_object + dt.timedelta(-2)\n",
    "    end_date = date_object + dt.timedelta(+2)\n",
    "    \n",
    "    start_date = start_date.strftime(\"%Y-%m-%d\")\n",
    "    end_date = end_date.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    make_dir(fire_name)\n",
    "\n",
    "    EXTENT = [lon - 0.5, lat - 0.5, lon + 0.5, lat + 0.5]\n",
    "    START = start_date\n",
    "    END = end_date\n",
    "    \n",
    "    satellites = ['SNPP','NOAA20']\n",
    "    for s in satellites:\n",
    "        s3_links, files = fetch_data(s)\n",
    "        \n",
    "        if s=='SNPP': products = ['VNP02IMG','VNP03IMG']\n",
    "        elif s=='NOAA20': products = ['VJ102IMG','VJ103IMG']\n",
    "        \n",
    "        run_fire_algorithm() #using level 1 data\n",
    "\n",
    "        #run_custom_fire_algorithm(s3_links, files) #using level 1 data\n",
    "        #run_viirs_l2_fire_algoithm(s3_links, files) #using level 2 data\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "196d947b-e310-4ff9-b59f-5fd740e36a34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1718913166.3466496"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22ee2fce-92e5-4d52-87ea-109e9147434a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00793808764881558"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current = time.time()\n",
    "elapsed = (current - start) / 60 /60 \n",
    "elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b341bf56-0dac-4655-814e-25198562fdf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python candidatesenv",
   "language": "python",
   "name": "candidates-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
